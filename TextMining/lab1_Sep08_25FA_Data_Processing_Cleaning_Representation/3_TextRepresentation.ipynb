{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1535fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries here that you need for different processing steps\n",
    "import nltk\n",
    "import csv\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f20594",
   "metadata": {},
   "source": [
    "## Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130443b5",
   "metadata": {},
   "source": [
    "### Word-to-vector\n",
    "\n",
    "You can implement any or many of these approaches to try out which one produces the best results ultimately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb745bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read a dataframe for understanding text-representation\n",
    "data_df = pd.read_csv(\"Dataset/covid.csv\")\n",
    "\n",
    "print (\"Data set: \", len(data_df))\n",
    "\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd950da",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56339c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explaining one-hot-encoding using one of the instances from the dataset. \n",
    "# Remember we talked about the sparsity in this approach and how this could be bad design for very large datasets.\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "docs = data_df.iloc[6][\"OriginalTweet\"].split()\n",
    "print(len(docs), docs, \"\\n\")\n",
    "\n",
    "# split documents to tokens\n",
    "tokens_docs = [doc.split(\" \") for doc in docs]\n",
    "\n",
    "# convert list of token-lists to one flat list of tokens\n",
    "# and then create a dictionary that maps word to id of word,\n",
    "\n",
    "# For large-scale dataset\n",
    "# all_tokens = itertools.chain.from_iterable(tokens_docs)\n",
    "\n",
    "# More intuitive way, but not optimal for large-scale input\n",
    "all_tokens = [token for doc in tokens_docs for token in doc]\n",
    "\n",
    "word_to_id = {token: idx for idx, token in enumerate(set(all_tokens))}\n",
    "print(len(word_to_id), word_to_id, \"\\n\")\n",
    "\n",
    "# convert token lists to token-id lists\n",
    "token_ids = [[word_to_id[token] for token in tokens_doc] for tokens_doc in tokens_docs]\n",
    "\n",
    "# convert list of token-id lists to one-hot representation\n",
    "vec = OneHotEncoder(categories=\"auto\")\n",
    "X = vec.fit_transform(token_ids)\n",
    "X = X.toarray()\n",
    "X\n",
    "# print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbefc3b4",
   "metadata": {},
   "source": [
    "### Bag Of Words BOW- CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847f3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As discussed about the CountVectorizer, below is implemented for only a set of instances for exploration \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# text = [\"i love nlp. nlp is so cool\"]\n",
    "text = data_df.iloc[1:4][\"OriginalTweet\"]\n",
    "print(text, \"\\n\")\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "print(vectorizer.vocabulary_, \"\\n\")\n",
    "# encode document\n",
    "\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "\n",
    "print(vector.shape, \"\\n\") \n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8965807b",
   "metadata": {},
   "source": [
    "### Term Frequency- Inverse Document Frequency (TF-IDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As discussed about the TF-IDF, below is implemented for only a set of instances for exploration \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# For more simple examples:\n",
    "# text1 = ['i love nlp', 'nlp is so cool', \n",
    "# 'nlp is all about helping machines process language', \n",
    "# 'this tutorial is on basic nlp technique']\n",
    "# print(text1)\n",
    "\n",
    "text1 = data_df.iloc[1:6][\"OriginalTweet\"].tolist()\n",
    "print(text1)\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "txt_fitted = tf.fit(text1)\n",
    "txt_transformed = txt_fitted.transform(text1)\n",
    "\n",
    "idf = tf.idf_\n",
    "print(dict(zip(txt_fitted.get_feature_names_out(), idf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2957276c",
   "metadata": {},
   "source": [
    "### UniGrams, BiGrams, TriGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "def ngram_convertor(sentence, n=3):\n",
    "    ngram_sentence = ngrams(sentence.split(), n)\n",
    "    for item in ngram_sentence:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de67d214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call this function with different values of sentence and N to see the grams building from the sentence.\n",
    "sentence = data_df.iloc[2][\"OriginalTweet\"]\n",
    "print(sentence)\n",
    "N = 3\n",
    "ngram_convertor(sentence,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18666f49-0fff-4e8b-a118-bf73956e7370",
   "metadata": {},
   "source": [
    "## Discussion Questions\n",
    "\n",
    "- Between one-hot encoding, count-based bag-of-words, and TF-IDF, which representation would you expect to work better for document classification, and why?\n",
    "    - Do you think any of these representations is always superior to the others across different NLP tasks and datasets? Why or why not?\n",
    "- TF-IDF relies on statistics from the training corpus. What potential issues might arise when applying the model to unseen documents? How could these issues be mitigated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4871c4-5987-43e2-91cc-0407ba190fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textmining",
   "language": "python",
   "name": "textmining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
